{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkXwBO+0dRdx6Mdvm1Cs1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdon021/From_Colab/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoYUrCM1Ng51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from mpl_toolkits import mplot3d\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SWVHGR5M6qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow import keras\n",
        "import keras.backend as kb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import datetime\n",
        "from numpy import linalg as LA"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5nbFKi6NaNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = [0, 0]\n",
        "cov = [[1, 0],\n",
        "       [0, 1]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqvfKtGGNcYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "united = np.random.multivariate_normal(mean, cov, 4000)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-Of9OD9Nrpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 4000\n",
        "xy_min = [0, 0]\n",
        "xy_max = [1, 1]\n",
        "data = np.random.uniform(low = xy_min, high = xy_max, size=(n, 2))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IybOJh2_NyVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def same_kernel(X, lam):\n",
        "#   Kernel_matrix = [np.exp(-np.square(LA.norm(np.delete(X[i]-X, obj=i,axis=0), axis=1))/lam) for i in range(len(X))]\n",
        "#   double_sum = tf.reduce_sum(Kernel_matrix)\n",
        "#   final = double_sum/(len(X)*(len(X)-1))\n",
        "#   return final\n",
        "\n",
        "# def dif_kernel(X,Y, lam):\n",
        "#   kernel_maxtrix = [np.exp(-np.square(LA.norm((X[i]-Y), axis=1)) /lam) for i in range(len(X))]\n",
        "#   double_sum = tf.reduce_sum(kernel_maxtrix)\n",
        "#   final = double_sum/(len(X)*(len(Y)))\n",
        "#   return final\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKTrUN3NiS5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def same_kernel(X, lam):\n",
        "#   Kernel_matrix = [np.exp(-np.square(LA.norm(np.delete(X[i]-X, obj=i,axis=0), axis=1))/lam) for i in range(len(X))]\n",
        "#   double_sum = tf.reduce_sum(Kernel_matrix)\n",
        "#   final = double_sum/(len(X)*(len(X)-1))\n",
        "#   return final"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMs_VWHknu7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def different_kernel(X,Y, lamb):\n",
        "#   return (tf.reduce_sum([tf.exp(-tf.square(tf.norm(X[i]-Y, axis=1))/lamb) for i in range(len(X))]))/(len(X)*(len(Y)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11NZsP7kip61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def same_kernels(X, lamb):\n",
        "#   return (tf.reduce_sum([tf.exp(-tf.square(tf.norm(X[i]-X, axis=1))/lamb) for i in range(len(X))]) - (len(X))/(len(X)*(len(X)-1)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDFSft4g-YOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def different_kernel(X,Y, lamb):\n",
        "  z1 = tf.expand_dims(X, 0)\n",
        "  z2 = tf.expand_dims(Y, 1)\n",
        "  return (tf.reduce_sum(tf.exp(-tf.reduce_sum(tf.math.squared_difference(z1, z2),axis= -1)/lamb)))/(len(X)*(len(Y)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVQCEcvw8HSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def same_kernels(X, lamb):\n",
        "  z1 = tf.expand_dims(X, 0)\n",
        "  z2 = tf.expand_dims(X, 1)\n",
        "  return (tf.reduce_sum(tf.exp(-tf.reduce_sum(tf.math.squared_difference(z1, z2),axis= -1)/lamb))-len(X))/(len(X)*(len(X)-1))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahDbl7w9N7XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def MMD(X, Y, lam):\n",
        "  return same_kernels(X, lam)+same_kernels(Y, lam)-(2*(different_kernel(X, Y, lam)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEVVYLUPOd86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(united, data, test_size = 0.25)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.25)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j36Wwb--OlWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape = (2,))\n",
        "x1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(10, activation='relu')(x1)\n",
        "outputs = layers.Dense(2, activation='linear')(x2)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Micd8IYvOs0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training dataset\n",
        "batch_size = 100"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwZbciKkOvVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tf.cast(X_train, tf.float32)\n",
        "Y_train = tf.cast(Y_train, tf.float32)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPZM6demOxqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val = tf.cast(X_val, tf.float32)\n",
        "Y_val = tf.cast(Y_val, tf.float32)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrAUWzogOyS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates a dataset with a separate element fro each row of the input tensor\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(buffer_size= 3000).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).shuffle(1000).batch(batch_size)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfcus2u4O8CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "epochs = 600"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9LluAfGO_oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_mse = np.array([])\n",
        "valid_mse = np.array([])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eSlN0OvPFhO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee6c066e-2514-4257-dd96-b8aa811d0e25"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch########################################### %d \\n\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # compute the loss value for this minibatch\n",
        "      # mse = Keras_loss_function(y_batch_train, linear)\n",
        "      # MMD_loss = same_kernels(linear, 0.5)+same_kernels(y_batch_train, 0.5)-2* different_kernel(linear, y_batch_train, 0.5)\n",
        "      # Silverman_factor https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html\n",
        "      n = x_batch_train.shape[0]\n",
        "      d = x_batch_train.shape[1]\n",
        "      bandwidth = (n*(d + 2) /4.)**(-1./ (d + 4)) # silverman_factor\n",
        "      # bandwidth = n**(-1./ ( d + 4)) # scotts_factor\n",
        "      # print(bandwidth)\n",
        "      MMD_loss = MMD(linear, y_batch_train, bandwidth)\n",
        "    grads = tape.gradient(MMD_loss, model.trainable_weights)\n",
        "    # print(grads)\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(optimizer.weights)\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training loss (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(MMD_loss))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training loss (for one batch) at step 10: 0.0086\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0016\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 45 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0027\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 46 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0081\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 47 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0018\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0038\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0021\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 48 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0149\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 49 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0023\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0102\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 50 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0086\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0016\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 51 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0042\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 52 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0018\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0083\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 53 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0018\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0044\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 54 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0002\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0104\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 55 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0028\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 56 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0052\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 57 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0109\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 58 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0053\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0149\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 59 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0048\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0004\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 60 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 61 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 62 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0017\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0175\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 63 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0039\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 64 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0126\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0140\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 65 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0023\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 66 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0096\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0041\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 67 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0017\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0054\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 68 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0050\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0037\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 69 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0095\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 70 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0018\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 71 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0022\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 72 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0050\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0052\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0023\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 73 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0040\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0050\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 74 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0283\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 75 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0028\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 76 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0097\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0066\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0033\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 77 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0078\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 78 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 79 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0017\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0021\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 80 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0052\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0077\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 81 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0034\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0061\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0081\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 82 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0026\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 83 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0386\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0044\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 84 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0119\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0038\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0084\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 85 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0032\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0007\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 86 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0107\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 87 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0022\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 88 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0096\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0020\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 89 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0032\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0037\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0007\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 90 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0081\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0108\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 91 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0122\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 92 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 93 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0099\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0047\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 94 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 95 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0022\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0108\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 96 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0060\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0070\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0074\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 97 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0012\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 98 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0027\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0166\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 99 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 100 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0107\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0005\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 101 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0082\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0069\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 102 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0003\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 103 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0005\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0072\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0021\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 104 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0142\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 105 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0006\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 106 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0099\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0026\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0041\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 107 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0048\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 108 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0113\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0062\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 109 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0131\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0043\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 110 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0047\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 111 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0042\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0008\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 112 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0072\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0043\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 113 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0128\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0036\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 114 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0051\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0001\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 115 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0019\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0131\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 116 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0117\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 117 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0017\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0222\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 118 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0056\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0005\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 119 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0075\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0032\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 120 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0018\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0038\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 121 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0062\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 122 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0038\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 123 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0012\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0061\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 124 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 125 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0026\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0037\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 126 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0044\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0060\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 127 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0072\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 128 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0018\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0065\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 129 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 130 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0018\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 131 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0023\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0047\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0039\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 132 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0067\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0016\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0001\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 133 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0116\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0132\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 134 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0060\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 135 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0000\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 136 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0053\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 137 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0023\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0033\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 138 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0103\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 139 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 140 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0061\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 141 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0088\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 142 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0314\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 143 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0040\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 144 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 145 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0059\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0094\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 146 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0023\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0057\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0032\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 147 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0008\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0179\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0007\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 148 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0101\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0283\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 149 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0106\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 150 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0071\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0016\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 151 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0203\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0067\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0008\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 152 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0060\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 153 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0037\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 154 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0054\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0075\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 155 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0074\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 156 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0036\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0069\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 157 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0058\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0014\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 158 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0051\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0010\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 159 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 160 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0042\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0003\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 161 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0070\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0040\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0017\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 162 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0064\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 163 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0052\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 164 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0033\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0047\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 165 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0044\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 166 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0000\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 167 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0015\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0049\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0087\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 168 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0104\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0034\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0039\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 169 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0017\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0036\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 170 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 171 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0170\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 172 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0041\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 173 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0020\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0053\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 174 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0054\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 175 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0072\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0001\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 176 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0087\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 177 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0010\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0129\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0145\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 178 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0066\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0007\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 179 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0033\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 180 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 181 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0003\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0051\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 182 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0017\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0200\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 183 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0018\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 184 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0060\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0044\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0014\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 185 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 186 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0054\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 187 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0058\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0060\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0019\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 188 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0314\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0026\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 189 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0054\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 190 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0058\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 191 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0032\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 192 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0163\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 193 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0032\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0028\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0053\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 194 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0061\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 195 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0091\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 196 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0144\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 197 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0010\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0114\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 198 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0043\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0022\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 199 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 200 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0070\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0042\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0067\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 201 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0005\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0040\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 202 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0069\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 203 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0076\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0013\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0045\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 204 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0077\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 205 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0043\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 206 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0011\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 207 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0069\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0149\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 208 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0088\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0010\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 209 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0034\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 210 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0065\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0028\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 211 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0063\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0070\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0077\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 212 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0023\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0196\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0001\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 213 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0008\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0118\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0054\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 214 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0301\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 215 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 216 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0060\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 217 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0034\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 218 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0007\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0039\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 219 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0032\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0186\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 220 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0000\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0067\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 221 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0068\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 222 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 223 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0010\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0013\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 224 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0057\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0065\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 225 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0024\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0019\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0021\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 226 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0042\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0026\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0019\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 227 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0018\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0087\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 228 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0239\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 229 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0039\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0149\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 230 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0070\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0052\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0025\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 231 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 232 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0060\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 233 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0065\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 234 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0057\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0049\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 235 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0060\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0221\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0044\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 236 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0036\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0038\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 237 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0155\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0007\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 238 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 239 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0045\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0140\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 240 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0097\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 241 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0201\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0067\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 242 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0004\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 243 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0032\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0050\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0053\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 244 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0121\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0019\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 245 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 246 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0059\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0056\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 247 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0079\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 248 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0028\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 249 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0089\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 250 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0005\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0018\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0042\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 251 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0007\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 252 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 253 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 254 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0039\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 255 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0003\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0040\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 256 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0069\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 257 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0067\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 258 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0270\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0051\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0025\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 259 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0009\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0004\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 260 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0000\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0032\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 261 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0033\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 262 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0048\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0060\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 263 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0058\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0087\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 264 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0107\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0094\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 265 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0103\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 266 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0034\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0095\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0008\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 267 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0018\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0087\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0049\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 268 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0027\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 269 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0032\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0132\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0074\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 270 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0020\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 271 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0010\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 272 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0065\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0047\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 273 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0000\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 274 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 275 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0066\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0045\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0045\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 276 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0069\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0034\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0065\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 277 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0071\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0064\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 278 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0043\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0177\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 279 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0006\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 280 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0011\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0056\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 281 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0062\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 282 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0179\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 283 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0125\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0032\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 284 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0126\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 285 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0066\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0038\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 286 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0056\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0041\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 287 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0178\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 288 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0033\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0133\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 289 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0021\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 290 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0153\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0017\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 291 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0042\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0135\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0125\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 292 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0069\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0066\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0117\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 293 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0044\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0054\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 294 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0008\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 295 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0130\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0060\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0042\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 296 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 297 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0070\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 298 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0064\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0125\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 299 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0013\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0014\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 300 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0066\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 301 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0009\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 302 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0093\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0188\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0052\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 303 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 304 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 305 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0095\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0059\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 306 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0072\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0039\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0067\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 307 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0174\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 308 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0192\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 309 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0061\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0026\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0044\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 310 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 311 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0090\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0054\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0089\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 312 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0326\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 313 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0053\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0068\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 314 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0065\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0075\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 315 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0009\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0074\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 316 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0023\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0032\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 317 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0000\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0010\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 318 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0059\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0043\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0078\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 319 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0025\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 320 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0054\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0054\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0070\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 321 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0003\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 322 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0054\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0017\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0039\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 323 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 324 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0054\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0079\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0006\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 325 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0079\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0001\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 326 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0060\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 327 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0044\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0078\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 328 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0052\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0110\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 329 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 330 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0019\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0009\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 331 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0119\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 332 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0006\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 333 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0073\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0080\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 334 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0061\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0032\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 335 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0043\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0043\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 336 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0027\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 337 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0112\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 338 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0045\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0050\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 339 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 340 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0013\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 341 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0003\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 342 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0038\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 343 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0057\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 344 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0017\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0053\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 345 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0039\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0492\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 346 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0031\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0049\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 347 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0042\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 348 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0043\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 349 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0037\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0005\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 350 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0052\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 351 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 352 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0152\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0090\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 353 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0026\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0140\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0010\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 354 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0277\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0025\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 355 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0000\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0052\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 356 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0053\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0123\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0058\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 357 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0009\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0034\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 358 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0034\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0072\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 359 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0036\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 360 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 361 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 362 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 363 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0048\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0038\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 364 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0157\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 365 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0180\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0112\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0032\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 366 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0073\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0017\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 367 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0002\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0119\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 368 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0017\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0000\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 369 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0013\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 370 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0020\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0019\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 371 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0024\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0071\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 372 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0080\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 373 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0023\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0103\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0093\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 374 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0002\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0012\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0020\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 375 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0032\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0023\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 376 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0060\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0165\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 377 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0031\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0025\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 378 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0034\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0079\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 379 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0032\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0079\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0049\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 380 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0054\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0022\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0249\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 381 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0042\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0061\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 382 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0172\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 383 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 384 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0226\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0015\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0061\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 385 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0005\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0034\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 386 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0070\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0000\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 387 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0060\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0059\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0098\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 388 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 389 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0016\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 390 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0067\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0171\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 391 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0099\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0053\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 392 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0019\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 393 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0007\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0047\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 394 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0025\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 395 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0064\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 396 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0162\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 397 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0002\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0013\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 398 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0071\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 399 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0043\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0068\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 400 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0063\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 401 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0009\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0003\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0061\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 402 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0009\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0183\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 403 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0074\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 404 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0054\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0060\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0010\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 405 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0123\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0079\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 406 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0031\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0052\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 407 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0048\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 408 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0125\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 409 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0056\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 410 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0026\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0043\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 411 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0053\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 412 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0058\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0064\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 413 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0066\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0042\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 414 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0071\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 415 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0060\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0044\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 416 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 417 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0003\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 418 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0032\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 419 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0060\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 420 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0040\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 421 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0032\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0102\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 422 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0003\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0039\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 423 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0052\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0060\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 424 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0015\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 425 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0053\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0062\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 426 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0072\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0081\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0065\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 427 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0053\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0023\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0050\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 428 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0018\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0114\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0028\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 429 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0057\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0025\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0040\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 430 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0042\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0011\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 431 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0032\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0061\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 432 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0054\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 433 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0068\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0052\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0025\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 434 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0057\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0085\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 435 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 436 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 437 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0091\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 438 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0070\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 439 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0002\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0042\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0059\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 440 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0028\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0086\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 441 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0013\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0004\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 442 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0079\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0006\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 443 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0059\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0033\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0033\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 444 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0017\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0006\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 445 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0073\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0020\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 446 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0020\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0003\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 447 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0043\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 448 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 449 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0077\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 450 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0028\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0064\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 451 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0050\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0020\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 452 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0007\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0053\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 453 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0043\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0067\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 454 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0026\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0090\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0040\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 455 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0083\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 456 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0079\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 457 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0049\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 458 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0021\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 459 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0066\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 460 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0080\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0023\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 461 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0048\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0061\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0064\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 462 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0032\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0058\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 463 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0056\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 464 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0076\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0020\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 465 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0054\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0085\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0047\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 466 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 467 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0057\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0037\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 468 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0018\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0093\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 469 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0054\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 470 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0010\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0026\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0053\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 471 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0070\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0047\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0062\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 472 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0024\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0048\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0023\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 473 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0071\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 474 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0026\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0023\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 475 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0133\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0056\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 476 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0031\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0043\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0064\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 477 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0027\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0073\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 478 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0057\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0095\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 479 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0027\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0068\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 480 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0050\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0100\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0094\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 481 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0000\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0114\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0063\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 482 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0040\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 483 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0050\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 484 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0071\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0125\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 485 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0026\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0020\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0038\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 486 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0058\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 487 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0072\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0051\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0145\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 488 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0054\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0053\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0025\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 489 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0010\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 490 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0015\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 491 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0071\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0111\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 492 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0133\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0047\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 493 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0022\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0013\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0114\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 494 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0005\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0017\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 495 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0036\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0040\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 496 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 497 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0219\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 498 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0043\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0086\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 499 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0044\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0049\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0077\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 500 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0090\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 501 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0034\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0173\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0043\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 502 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0082\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0050\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0025\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 503 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0048\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 504 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0000\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 505 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0062\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0040\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 506 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0069\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0029\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 507 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0015\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0034\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 508 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0043\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0039\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0002\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 509 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0151\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0101\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 510 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0063\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 511 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0138\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 512 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0157\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 513 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 514 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0027\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0057\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 515 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0236\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0106\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 516 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0012\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0010\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 517 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0052\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 518 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0145\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 519 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0018\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0353\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 520 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0028\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0043\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 521 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0132\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0073\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 522 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0068\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0053\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 523 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0108\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0008\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 524 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0037\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0037\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 525 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0080\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0062\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 526 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 527 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0007\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0042\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 528 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0098\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0050\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 529 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0054\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 530 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0047\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0080\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0047\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 531 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0128\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0046\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0018\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 532 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0070\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 533 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0090\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0074\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0054\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 534 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0063\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 535 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0025\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0008\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 536 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0083\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0035\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 537 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0069\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0008\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 538 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0032\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 539 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0067\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 540 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0059\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0174\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0037\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 541 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0057\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0042\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0015\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 542 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0069\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0001\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 543 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0023\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0073\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 544 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0001\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0017\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 545 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0047\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 546 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0027\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 547 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0003\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 548 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0100\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0036\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 549 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0178\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0042\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0007\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 550 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0072\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0051\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0028\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 551 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0054\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 552 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0087\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0084\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 553 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0037\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0065\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 554 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0026\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0023\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0031\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 555 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0029\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0034\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0033\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 556 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0056\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0070\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0012\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 557 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0002\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0054\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0061\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 558 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0045\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0037\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 559 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0209\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0018\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0019\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 560 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0053\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 561 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0126\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0052\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 562 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0052\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0052\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 563 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0015\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0014\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 564 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0061\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0010\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 565 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0080\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0002\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 566 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0071\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0037\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 567 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0025\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0022\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0175\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 568 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0060\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0001\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 569 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0213\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0066\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 570 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0026\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 571 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0037\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0004\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 572 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0065\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0021\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 573 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0072\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0079\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 574 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0019\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0023\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 575 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0055\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0030\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 576 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0032\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0076\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 577 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0058\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0005\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 578 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0163\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0064\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 579 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0248\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0042\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0078\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 580 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0024\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 581 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0036\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0046\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 582 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0088\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0010\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 583 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0050\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0096\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0023\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 584 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0053\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0068\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0001\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 585 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0061\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0163\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 586 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0070\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0059\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0077\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 587 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0015\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0055\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 588 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0051\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0040\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0088\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 589 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 590 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0017\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0052\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0199\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 591 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0027\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0037\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0013\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 592 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0024\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 593 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0001\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0043\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0022\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 594 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0037\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0022\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0039\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 595 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0091\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0077\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0041\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 596 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0021\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0020\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0051\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 597 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0148\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0077\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0104\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 598 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0020\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: -0.0057\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 599 \n",
            "\n",
            "Training loss (for one batch) at step 0: -0.0041\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: -0.0031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0134\n",
            "seen so far: 2100 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1XwmcEyVTTV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c23aca81-4dfd-44eb-e2e2-511275a7a898"
      },
      "source": [
        "xx1, xx2 = X_test.T\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.plot(xx1, xx2, 'o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4950ddbb00>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Q6tBYvLA8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "52a53e7f-87ca-4757-b8af-ecc20f691328"
      },
      "source": [
        "prediction = model.predict(X_test)\n",
        "f1, f2= prediction.T\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(f1, f2, 'o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7f495036b550>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn_1wwXhLNYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "790368ba-23cb-4e75-862f-e9958d88c510"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.plot(f1, f2, 'o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f49503fa748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRfDFjqGLU4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y1,y2 = Y_test.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aiM2-S3Lb17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "fd59fd3d-2f79-4ceb-8bb2-931a0913afcf"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.plot(y1,y2, 'o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4ca00c8860>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1v6Co6QkWIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_mmd = np.array([])\n",
        "valid_mmd = np.array([])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pffTeq-ECgH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape = (2,))\n",
        "x1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(10, activation='relu')(x1)\n",
        "outputs = layers.Dense(2, activation='linear')(x2)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "epochs = 1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev41KhWzDg63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in 10**np.linspace(1, -4, 30):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwVfr2XYoTXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "589f8d49-6135-4a5b-810c-f64d9bb18056"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch########################################### %d \\n\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # compute the loss value for this minibatch\n",
        "      # mse = Keras_loss_function(y_batch_train, linear)\n",
        "      # MMD_loss = same_kernels(linear, 0.5)+same_kernels(y_batch_train, 0.5)-2* different_kernel(linear, y_batch_train, 0.5)\n",
        "      # weights_sum = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights] # this regularized bias term, should not \n",
        "      weights_sum = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights if 'bias' not in w.name] # exclude bias term\n",
        "      # reg_losses = kb.sum(kb.abs(model.trainable_weights))\n",
        "      # print(weights_sum)\n",
        "      MMD_loss = MMD(linear, y_batch_train, 2)\n",
        "      # print(MMD_loss)\n",
        "      MMD_loss_regu = MMD_loss + 0.02*tf.reduce_sum(weights_sum)\n",
        "      # print(MMD_loss_regu)\n",
        "    grads = tape.gradient(MMD_loss_regu, model.trainable_weights)\n",
        "    # add gradient penalty terms to the gradient\n",
        "    grads_penalty = [ gradient_matrix + tf.square(tf.norm(gradient_matrix, ord ='euclidean', axis = 0)-1) for gradient_matrix in grads]\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads_penalty, model.trainable_weights))\n",
        "    # print(optimizer.weights)\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training MMD (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(MMD_loss_regu))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))\n",
        "\n",
        "  # Run a validation loop at the end of each epoch\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_linear = model(x_batch_val, training = False)\n",
        "    weights_sum_val = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights if 'bias' not in w.name]\n",
        "    MMD_loss_val = MMD(val_linear, y_batch_val, 2)\n",
        "    MMD_loss_regu_val = MMD_loss + 0.02*tf.reduce_sum(weights_sum)\n",
        "\n",
        "  # print MSE for validation set\n",
        "  print(\"validation MMD: %.4f\" % (float(MMD_loss_regu_val)))\n",
        "  training_mmd = np.append(training_mmd, float(MMD_loss_regu))\n",
        "  valid_mmd = np.append(valid_mmd, float(MMD_loss_regu_val))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch########################################### 0 \n",
            "\n",
            "Training MMD (for one batch) at step 0: 0.4163\n",
            "seen so far: 100 samples\n",
            "Training MMD (for one batch) at step 10: 0.4405\n",
            "seen so far: 1100 samples\n",
            "Training MMD (for one batch) at step 20: 0.4933\n",
            "seen so far: 2100 samples\n",
            "validation MMD: 0.5835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjX8k9ZMDG4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a3b194fd-d8b8-4af5-ac18-1ca86bba834a"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch########################################### %d \\n\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # compute the loss value for this minibatch\n",
        "      # mse = Keras_loss_function(y_batch_train, linear)\n",
        "      # MMD_loss = same_kernels(linear, 0.5)+same_kernels(y_batch_train, 0.5)-2* different_kernel(linear, y_batch_train, 0.5)\n",
        "      # weights_sum = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights] # this regularized bias term, should not \n",
        "      weights_sum = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights if 'bias' not in w.name] # exclude bias term\n",
        "      # reg_losses = kb.sum(kb.abs(model.trainable_weights))\n",
        "      # print(weights_sum)\n",
        "      MMD_loss = MMD(linear, y_batch_train, 2)\n",
        "      # print(MMD_loss)\n",
        "      MMD_loss_regu = MMD_loss + 0.02*tf.reduce_sum(weights_sum)\n",
        "      # print(MMD_loss_regu)\n",
        "    grads = tape.gradient(MMD_loss_regu, model.trainable_weights)\n",
        "    # print(grads)\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(optimizer.weights)\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training MMD (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(MMD_loss_regu))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))\n",
        "\n",
        "  # Run a validation loop at the end of each epoch\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_linear = model(x_batch_val, training = False)\n",
        "    weights_sum_val = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights if 'bias' not in w.name]\n",
        "    MMD_loss_val = MMD(val_linear, y_batch_val, 2)\n",
        "    MMD_loss_regu_val = MMD_loss + 0.02*tf.reduce_sum(weights_sum)\n",
        "\n",
        "  # print MSE for validation set\n",
        "  print(\"validation MMD: %.4f\" % (float(MMD_loss_regu_val)))\n",
        "  training_mmd = np.append(training_mmd, float(MMD_loss_regu))\n",
        "  valid_mmd = np.append(valid_mmd, float(MMD_loss_regu_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch########################################### 0 \n",
            "\n",
            "validation MSE: 1.0548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWW4g0XzJVFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch########################################### %d \\n\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # compute the loss value for this minibatch\n",
        "      # mse = Keras_loss_function(y_batch_train, linear)\n",
        "      # MMD_loss = same_kernels(linear, 0.5)+same_kernels(y_batch_train, 0.5)-2* different_kernel(linear, y_batch_train, 0.5)\n",
        "      # weights_sum = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights] # this regularized bias term, should not \n",
        "      weights_sum = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights if 'bias' not in w.name] # exclude bias term\n",
        "      # reg_losses = kb.sum(kb.abs(model.trainable_weights))\n",
        "      # print(weights_sum)\n",
        "      MMD_loss = MMD(linear, y_batch_train, 2)\n",
        "      # print(MMD_loss)\n",
        "      MMD_loss_regu = MMD_loss + 0.02*tf.reduce_sum(weights_sum)\n",
        "      # print(MMD_loss_regu)\n",
        "    break\n",
        "    grads = tape.gradient(MMD_loss_regu, model.trainable_weights)\n",
        "    # print(grads)\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(optimizer.weights)\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training MMD (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(MMD_loss_regu))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))\n",
        "\n",
        "  # Run a validation loop at the end of each epoch\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_linear = model(x_batch_val, training = False)\n",
        "    weights_sum_val = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights if 'bias' not in w.name]\n",
        "    MMD_loss_val = MMD(val_linear, y_batch_val, 2)\n",
        "    MMD_loss_regu_val = MMD_loss + 0.02*tf.reduce_sum(weights_sum)\n",
        "\n",
        "  # print MSE for validation set\n",
        "  print(\"validation MMD: %.4f\" % (float(MMD_loss_regu_val)))\n",
        "  training_mmd = np.append(training_mmd, float(MMD_loss_regu))\n",
        "  valid_mmd = np.append(valid_mmd, float(MMD_loss_regu_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfydhfFLMxsx",
        "colab_type": "text"
      },
      "source": [
        "### 1-D Uniform to 2D guassian\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNXYcFm3KCwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 4000\n",
        "xy_min = [0]\n",
        "xy_max = [1]\n",
        "data2 = np.random.uniform(low = xy_min, high = xy_max, size=(n, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt1FOvW7NMQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training dataset\n",
        "batch_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWSa_LGPNR1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(data2, united, test_size = 0.25)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4XRdiM-Nbvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tf.cast(X_train, tf.float32)\n",
        "Y_train = tf.cast(Y_train, tf.float32)\n",
        "X_val = tf.cast(X_val, tf.float32)\n",
        "Y_val = tf.cast(Y_val, tf.float32)\n",
        "X_test = tf.cast(X_test, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-XljhI9Nfq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates a dataset with a separate element fro each row of the input tensor\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(buffer_size= 3000).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).shuffle(1000).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9x-X6RANhwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IRDR_qlKCFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape = (1,))\n",
        "x1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(10, activation='relu')(x1)\n",
        "outputs = layers.Dense(2, activation='linear')(x2)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am44e1WCNlVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b771eb7-16a6-45fa-b014-1d9411467fb2"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch########################################### %d \\n\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # compute the loss value for this minibatch\n",
        "      # weights_sum = [tf.reduce_sum(tf.square(w)) for w in model.trainable_weights]\n",
        "      # reg_losses = kb.sum(kb.abs(model.trainable_weights))\n",
        "      # print(weights_sum)\n",
        "      MMD_loss = MMD(linear, y_batch_train, 2)\n",
        "      # print(MMD_loss)\n",
        "      # MMD_loss_regu = MMD_loss + 0.02*tf.reduce_sum(weights_sum)\n",
        "      # print(MMD_loss_regu)\n",
        "    grads = tape.gradient(MMD_loss, model.trainable_weights)\n",
        "    # print(grads)\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(optimizer.weights)\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training loss (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(MMD_loss))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch########################################### 0 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.3927\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.3449\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.2982\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 1 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.2991\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.3262\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.4102\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 2 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.2827\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.3004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.2648\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 3 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.2365\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.2251\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.3236\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 4 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.2779\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.2501\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.2108\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 5 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.2360\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.1885\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.1640\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 6 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.1461\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.1446\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.1595\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 7 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.1303\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.1842\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0816\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 8 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.0906\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0993\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.1078\n",
            "seen so far: 2100 samples\n",
            "\n",
            "Start of epoch########################################### 9 \n",
            "\n",
            "Training loss (for one batch) at step 0: 0.1454\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.0827\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.0847\n",
            "seen so far: 2100 samples\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}